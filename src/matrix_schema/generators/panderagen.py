"""
Pandera schema generator for LinkML schemas.

This generator creates Pandera schemas from LinkML schemas with customizable
class selection and specialized handling for union types.
"""

import os
import subprocess
import tempfile
from pathlib import Path
from typing import List, Optional, Dict, Any, Set

import click
from jinja2 import Template
from linkml_runtime.linkml_model import SchemaDefinition, ClassDefinition, SlotDefinition
from linkml_runtime.utils.schemaview import SchemaView

try:
    from linkml.generators.generator import Generator, shared_arguments
except ImportError:
    # Fallback for older versions or missing linkml.generators
    class Generator:
        generatorname = "panderagen"
        valid_formats = ["py"]
        visit_all_class_slots = True
        
        def __init__(self, schema, **kwargs):
            self.schema = schema
            
    def shared_arguments(cls):
        def decorator(f):
            f = click.argument('yamlfile')(f)
            return f
        return decorator


class PanderaGenerator(Generator):
    """Generator for Pandera schemas from LinkML schemas."""
    
    generatorname = os.path.basename(__file__)
    valid_formats = ["py"]
    visit_all_class_slots = True
    
    def __init__(
        self,
        schema: str | SchemaDefinition,
        classes: Optional[List[str]] = None,
        **kwargs
    ):
        super().__init__(schema, **kwargs)
        self.schemaview = SchemaView(schema)
        # Preserve the order of classes as specified
        self.target_classes = classes if classes else []
        
    def serialize(self, **kwargs) -> str:
        """Generate Pandera schema code."""
        # Get template
        template_str = self._get_template()
        template = Template(template_str)
        
        # Prepare data for template
        template_data = self._prepare_template_data()
        
        # Render template
        code = template.render(**template_data)
        
        # Format with ruff if available (disabled for now due to template indentation issues)
        # return self._format_code(code)
        return code
    
    def _format_code(self, code: str) -> str:
        """Format code using ruff if available."""
        try:
            # Create a temporary file to format
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(code)
                temp_path = f.name
            
            try:
                # Try to format with ruff
                result = subprocess.run(
                    ['ruff', 'format', '--stdin-filename', temp_path, '-'],
                    input=code,
                    text=True,
                    capture_output=True,
                    check=False
                )
                
                if result.returncode == 0:
                    return result.stdout
                else:
                    # If ruff fails, return original code
                    return code
                    
            finally:
                # Clean up temp file
                os.unlink(temp_path)
                
        except (FileNotFoundError, subprocess.SubprocessError):
            # If ruff is not available, return original code
            return code
    
    def _get_template(self) -> str:
        """Get the Jinja template for Pandera generation."""
        return '''# THIS FILE IS AUTOGENERATED. DO NOT EDIT MANUALLY.
"""
Auto-generated Pandera schemas from LinkML schema.
"""

try:
    import pyspark.sql.types as T
    import pandera.pandas as pa
    from ..utils.pandera_utils import Column, DataFrameSchema
    # Import the enums from the pydantic model
    from .matrix_schema_pydantic import (
        PredicateEnum,
        NodeCategoryEnum,
        KnowledgeLevelEnum,
        AgentTypeEnum
    )
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    T = None
    pa = None
    Column = None
    DataFrameSchema = None
    PredicateEnum = None
    NodeCategoryEnum = None
    KnowledgeLevelEnum = None
    AgentTypeEnum = None
    DEPENDENCIES_AVAILABLE = False
    IMPORT_ERROR = str(e)

{% for class_name, class_info in classes.items() %}
def get_{{ class_info.function_name }}(validate_enumeration_values: bool = True):
    """Get the Pandera schema for {{ class_name }} validation.
    
    Returns a universal schema that works with both pandas and PySpark DataFrames.
    The actual schema type is determined at validation time based on the DataFrame type.
    """
    if not DEPENDENCIES_AVAILABLE:
        error_msg = f"Dependencies not available for Pandera schema. Original error: {IMPORT_ERROR if 'IMPORT_ERROR' in globals() else 'Unknown'}. Install with: pip install 'pandera[pyspark]' pyspark"
        raise ImportError(error_msg)
{%- set enum_slots = [] -%}
{%- for slot_name, slot_info in class_info.slots.items() -%}
{%- if slot_info.enum_checks -%}
{%- set _ = enum_slots.append((slot_name, slot_info.enum_checks)) -%}
{%- endif -%}
{%- endfor -%}
{%- if enum_slots %}

    {% for slot_name, enum_check in enum_slots -%}
    if validate_enumeration_values:
        {{ slot_name }}_checks = [{{ enum_check }}]
    else:
        {{ slot_name }}_checks = []

    {% endfor -%}
{%- endif -%}

    return DataFrameSchema(
        columns={
{%- for slot_name, slot_info in class_info.slots.items() %}
            "{{ slot_name }}": Column({{ slot_info.pyspark_type }}, nullable={{ slot_info.nullable | title }}{% if slot_info.enum_checks %},
                             checks={{ slot_name }}_checks{% endif %}),
{%- endfor %}
        },
{%- if class_info.unique_fields %}
        unique={{ class_info.unique_fields }},
{%- endif %}
        strict=True,
    )


{% endfor %}
'''
    
    def _prepare_template_data(self) -> Dict[str, Any]:
        """Prepare data for the Jinja template."""
        # Use OrderedDict to preserve the order of classes as specified in CLI
        from collections import OrderedDict
        classes_data = OrderedDict()
        
        for class_name in self.target_classes:
            class_def = self.schemaview.get_class(class_name)
            if not class_def:
                continue
                
            class_info = self._process_class(class_name, class_def)
            classes_data[class_name] = class_info
            
        return {
            "classes": classes_data,
            "schema_name": self.schemaview.schema.name,
        }
    
    def _process_class(self, class_name: str, class_def: ClassDefinition) -> Dict[str, Any]:
        """Process a single class definition."""
        # Get all slots for this class
        all_slots = self.schemaview.class_slots(class_name)
        
        slots_data = {}
        has_primary_knowledge_sources = False
        has_categories = False
        
        for slot_name in all_slots:
            slot_def = self.schemaview.get_slot(slot_name)
            if not slot_def:
                continue
                
            slot_info = self._process_slot(slot_def)
            slots_data[slot_name] = slot_info
            
            if slot_name == "primary_knowledge_sources":
                has_primary_knowledge_sources = True
            elif slot_name in ["category", "categories"]:
                has_categories = True
        
        # Generate function name (convert CamelCase to snake_case)
        function_name = self._to_snake_case(class_name) + "_schema"
        
        # Determine unique fields based on class name
        unique_fields = None
        if "node" in class_name.lower():
            unique_fields = '["id"]'
        elif "edge" in class_name.lower():
            unique_fields = '["subject", "predicate", "object"]'
            
        return {
            "function_name": function_name,
            "slots": slots_data,
            "unique_fields": unique_fields,
            "description": class_def.description,
        }
    
    def _process_slot(self, slot_def: SlotDefinition) -> Dict[str, Any]:
        """Process a single slot definition."""
        # Determine PySpark type
        pyspark_type = self._linkml_to_pyspark_type(
            slot_def.range if slot_def.range else "string", 
            slot_def.multivalued
        )
        
        # Determine nullability
        nullable = not slot_def.required if slot_def.required is not None else True
        
        # Generate enum checks if applicable
        enum_checks = None
        if slot_def.range:
            range_name = slot_def.range
            if range_name == "NodeCategoryEnum":
                enum_checks = "pa.Check.isin([category.value for category in NodeCategoryEnum])"
            elif range_name == "PredicateEnum":
                enum_checks = "pa.Check.isin([predicate.value for predicate in PredicateEnum])"
            elif range_name == "KnowledgeLevelEnum":
                enum_checks = "pa.Check.isin([level.value for level in KnowledgeLevelEnum])"
            elif range_name == "AgentTypeEnum":
                enum_checks = "pa.Check.isin([agent.value for agent in AgentTypeEnum])"
        
        return {
            "pyspark_type": pyspark_type,
            "nullable": nullable,
            "enum_checks": enum_checks,
            "description": slot_def.description,
            "multivalued": slot_def.multivalued,
        }
    
    def _get_pandera_type(self, slot_def: SlotDefinition) -> str:
        """Convert LinkML type to Pandera type."""
        if slot_def.range:
            range_type = slot_def.range
        else:
            range_type = "string"  # default
            
        # Handle multivalued slots
        if slot_def.multivalued:
            base_type = self._linkml_to_pandera_type(range_type)
            return f"object"  # Lists are stored as objects in pandas
        else:
            return self._linkml_to_pandera_type(range_type)
    
    def _linkml_to_pyspark_type(self, linkml_type: str, multivalued: bool = False) -> str:
        """Map LinkML types to PySpark types."""
        base_type_mapping = {
            "string": "T.StringType()",
            "integer": "T.IntegerType()",
            "float": "T.FloatType()", 
            "double": "T.DoubleType()",
            "boolean": "T.BooleanType()",
            "date": "T.StringType()",
            "datetime": "T.StringType()",
            "uri": "T.StringType()",
            "uriorcurie": "T.StringType()",
        }
        
        base_type = base_type_mapping.get(linkml_type.lower(), "T.StringType()")
        
        if multivalued:
            # For arrays, the nullable=False parameter applies to the array items, not the array itself
            return f"T.ArrayType({base_type}, False)"
        else:
            return base_type
    
    def _generate_checks(self, slot_def: SlotDefinition) -> List[str]:
        """Generate Pandera checks for a slot."""
        checks = []
        
        # Add pattern check if available
        if slot_def.pattern:
            checks.append(f'pa.Check.str_matches(r"{slot_def.pattern}")')
            
        # Add enum check if available  
        if hasattr(slot_def, 'permissible_values') and slot_def.permissible_values:
            values = [f'"{pv.text}"' for pv in slot_def.permissible_values.values()]
            checks.append(f'pa.Check.isin([{", ".join(values)}])')
            
        return checks
    
    def _to_snake_case(self, camel_str: str) -> str:
        """Convert CamelCase to snake_case."""
        result = []
        for i, char in enumerate(camel_str):
            if char.isupper() and i > 0:
                result.append('_')
            result.append(char.lower())
        return ''.join(result)


@click.command()
@click.argument('yamlfile')
@click.option(
    "--classes",
    "-c", 
    multiple=True,
    required=True,
    help="Classes to generate Pandera schemas for (can be specified multiple times)"
)
@click.option(
    "--output", 
    "-o", 
    help="Output file (default: stdout)"
)
def cli(yamlfile, classes, output):
    """Generate Pandera schemas from LinkML schema."""
    try:
        generator = PanderaGenerator(yamlfile, classes=list(classes))
        result = generator.serialize()
        
        if output:
            with open(output, 'w') as f:
                f.write(result)
            click.echo(f"Generated Pandera schemas written to {output}", err=True)
        else:
            click.echo(result)
            
    except Exception as e:
        raise click.ClickException(f"Error: {e}")


if __name__ == "__main__":
    cli()